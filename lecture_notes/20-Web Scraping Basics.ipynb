{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginning of this lecture is based on chapter 11 of *Automate the Boring Stuff with Python* by Al Sweigart\n",
    "\n",
    "# HTML Refresher\n",
    "\n",
    "HTML files are plain text files containing *tags*, which are words enclosed in angle brackets. Tags tell the browser how to format the web page. A starting tag and closing tag can enclose some text to form an element. The text (or inner HTML) is the content between the starting and closing tags.\n",
    "\n",
    "There are many different tags in HTML. Some of these tags have extra properties in the form of attributes within the angle brackets. For example, the `<a>` tag encloses text that should be a link.\n",
    "\n",
    "Some elements have an `id` attribute that is used to uniquely identify the element in the page. You will often instruct your programs to seek out an element by its id attribute, so figuring out an element’s id attribute using the browser’s developer tools is a common task in writing web scraping programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"<strong>Hej</strong> class!<br>\" > hej.html\n",
    "echo \"The <a href=\\\"https://github.com/datsoftlyngby/dat4sem2018spring-python\\\">Lecture Notes</a>.\" >> hej.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "open hej.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View a Page's HTML Sources\n",
    "\n",
    "Here, I will only describe how to use Firefox' development features.\n",
    "\n",
    "To view a page's sources right click on it and choose **View page source** which opens a new tab with the HTML sources.\n",
    "\n",
    "![screenshot](images/view_source_small.png)\n",
    "\n",
    "\n",
    "In Firefox, you can bring up the Web Developer Tools Inspector by pressing `CTRL-SHIFT-C` on Windows and Linux or by `CMD-OPTION-C` on OS X.\n",
    "\n",
    "![screenshot](images/inspector_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML with BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a module for parsing and extracting information from HTML sources. The module’s name is bs4. In case it is not already install to your VM, install it with `pip install beautifulsoup4`. While beautifulsoup4 is the name used for installation, to import BeautifulSoup you have to use `import bs4`.\n",
    "\n",
    "According to its documentation (https://www.crummy.com/software/BeautifulSoup/) *\"Beautiful Soup parses anything you give it, and does the tree traversal stuff for you. You can tell it \"Find all the links\", or \"Find all the links of class externalLink\", or \"Find all the links whose urls match \"foo.com\", or \"Find the table heading that's got bold text, then give me that text.\"\"*\n",
    "\n",
    "\n",
    "## Creating a BeautifulSoup Object from a Local HTML File\n",
    "\n",
    "The `bs4.BeautifulSoup()` function needs to be called with a string containing the HTML it will parse. The `bs4.BeautifulSoup()` function returns is a `BeautifulSoup` object.\n",
    "\n",
    "You can load a local HTML file and pass a file object to `bs4.BeautifulSoup()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "<!-- This is the example.html example file. -->\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Example Course Page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p>\n",
      "   Find the\n",
      "   <strong>\n",
      "    Lecture Notes\n",
      "   </strong>\n",
      "   on\n",
      "   <a href=\"https://github.com/HelgeCPH/get_things_done_with_python\">\n",
      "    Github\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      "  <p class=\"slogan\">\n",
      "   Getting Things Done with Python!\n",
      "  </p>\n",
      "  <p>\n",
      "   Tought by\n",
      "   <span id=\"lecturer\">\n",
      "    Helge\n",
      "   </span>\n",
      "   at\n",
      "   <span id=\"institution\">\n",
      "    CPH Business\n",
      "   </span>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "\n",
    "with open('./example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html, 'html5lib')\n",
    "print(type(soup))\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a BeautifulSoup Object from a Remote HTML File\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <link href=\"https://assets-cdn.github.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://avatars0.githubusercontent.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://avatars1.githubusercontent.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://avatars2.githubusercontent.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://avatars3.githubusercontent.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://github-cloud.s3.amazonaws.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://user-images.githubusercontent.com/\" rel=\"dns-prefetch\"/>\n",
      "  <link crossorigin=\"anonymous\" href=\"https://assets-cdn.github.com/assets/frameworks-6caac13806331f9675860df98a99b39a.css\" media=\"all\" rel=\"stylesheet\"/>\n",
      "  <link crossorigin=\"anonymous\" href=\"https://assets-cdn.github.com/assets/github-c4d5af25254cfc8bb30ae5cb6f074c97.css\" media=\"all\" rel=\"stylesheet\"/>\n",
      "  <link crossorigin=\"anonymous\" href=\"https://assets-cdn.github.com/assets/site-83dc1f7ebc9c7461fe1eab799b56c4c4.css\" media=\"all\" rel=\"stylesheet\"/>\n",
      "  <meta content=\"width=device-width\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   GitHub - datsoftlyngby/dat4sem2018spring-python: Repository containing the course material for the elective course \"Getting Things Done with Python\" (Spring 2018) at CPH Business Academy (www.cphbusiness.dk).\n",
      "  </title>\n",
      "  <meta content=\"GitHub is where people build software. More than 27 million people use GitHub to discover, fork, and contribute to over 80 million projects.\" name=\"descri\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('https://github.com/datsoftlyngby/dat4sem2018spring-python')\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html5lib')\n",
    "\n",
    "print(soup.prettify()[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding an Element with the `select()` Method\n",
    "\n",
    "You can retrieve HTML elements from a `BeautifulSoup` object by calling the `select()` method and passing a string of a CSS selector for the element you are looking for. Selectors are like regular expressions: They specify a pattern to look for, in this case, in HTML pages instead of general text strings.\n",
    "\n",
    "Common CSS selector patterns include:\n",
    "\n",
    "  * `soup.select('div')` ... selects all elements named `<div>`\n",
    "  * `soup.select('#lecturer')`  ... selects the element with an id attribute of author\n",
    "  * `soup.select('.notice')` ... selects all elements that use a CSS class attribute named notice\n",
    "  * `soup.select('div span')` ... selects all elements named ``<span>` that are within an element named `<div>`\n",
    "  * `soup.select('div > span')` ... selects all elements named `<span>` that are directly within an element named `<div>`, with no other element in between\n",
    "  * `soup.select('input[name]')` ... selects all elements named `<input>` that have a name attribute with any value\n",
    "  * `soup.select('input[type=\"button\"]')` ... selects all elements named `<input>` that have an attribute named type with value button\n",
    "  \n",
    "See more in the documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n",
      "<class 'bs4.element.Tag'>\n",
      "Helge\n",
      "<span id=\"lecturer\">Helge</span>\n",
      "{'id': 'lecturer'}\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "\n",
    "with open('./example.html') as f:\n",
    "    example_html = f.read()\n",
    "  \n",
    "\n",
    "soup = bs4.BeautifulSoup(example_html, 'html5lib')\n",
    "\n",
    "elems = soup.select('#lecturer')\n",
    "\n",
    "#print(soup.prettify())\n",
    "print(type(elems))\n",
    "print(len(elems))\n",
    "print(type(elems[0]))\n",
    "print(elems[0].getText())\n",
    "print(str(elems[0]))\n",
    "print(elems[0].attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Find the <strong>Lecture Notes</strong> on <a href=\"https://github.com/HelgeCPH/get_things_done_with_python\">Github</a>.</p>\n",
      "Find the Lecture Notes on Github.\n",
      "------------\n",
      "<p class=\"slogan\">Getting Things Done with Python!</p>\n",
      "Getting Things Done with Python!\n",
      "------------\n",
      "<p>Tought by <span id=\"lecturer\">Helge</span> at <span id=\"institution\">CPH Business</span> </p>\n",
      "Tought by Helge at CPH Business \n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "p_elems = soup.select('p')\n",
    "\n",
    "for el in p_elems:\n",
    "    # str(p_elems[0]), str(p_elems[1]),...\n",
    "    print(str(el))\n",
    "    print(el.getText())\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data from an Element’s Attributes\n",
    "\n",
    "The `get()` method for Tag objects makes it simple to access attribute values from an element. The method is passed a string of an attribute name and returns that attribute’s value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'id': 'lecturer'}.get('non_exisiting', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span id=\"lecturer\">Helge</span>\n",
      "lecturer\n",
      "True\n",
      "{'id': 'lecturer'}\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "with open('./example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html, 'html5lib')\n",
    "soup.find_all?\n",
    "span_elem = soup.select('span')[0]\n",
    "print(str(span_elem))\n",
    "print(span_elem.get('id'))\n",
    "print(span_elem.get('some_nonexistent_addr') == None)\n",
    "print(span_elem.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Scraping Events from a Page\n",
    "\n",
    "\n",
    "Ususally, you will use web scraping to collect information, which you cannot gather otherwise. For example, let's imagine we want to do some statistics about concerts in Copenhagen, their start times and their door prices.\n",
    "\n",
    "Since we cannot find an API or any other open dataset, we decide to scrape the publicly available homepage www.kultunaut.dk, which lists all possible events in Denmark. Concerts in Copenhagen are for example accessible here: http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&periode=&Genre=Musik\n",
    "\n",
    "**OBS** Many web pages are not built to support high traffic or they exlicitely discourage automatic access. Keep this in mind when writing your scraping tool.\n",
    "\n",
    "\n",
    "Considering our example, we have to first figure out how many events there are at all. We need this information, as events are given paginated, i.e., twenty events per page. The link given above only returns the link to the first page with the first twenty events. Out of the total amount of events we can generate the URLs for the subsequent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joachim example:\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&periode=&Genre=Musik')\n",
    "r.raise_for_status\n",
    "soup = bs4.BeautifulSoup(r.text, 'html5lib')\n",
    "\n",
    "elems = soup.find_all('td', {'style': 'background-color:#CBDCEE;color:#324669'})\n",
    "number = int(str(elems[0]).split(' ')[4].split('\\n')[0])\n",
    "\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>Showing 1-20 of 1339\n",
      "events  in Cph. and Frederiksberg <span style=\"white-space: nowrap;\"> from/after 16 Mar 2018</span> </b>\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&periode=&Genre=Musik')\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html5lib')\n",
    "\n",
    "elems = soup.find_all('td', attrs={'style': 'background-color:#CBDCEE;color:#324669'})[0].find('b')\n",
    "print(elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "def scrape_no_events(url):\n",
    "    \"\"\"\n",
    "    Find amount of pages to parse from the entry html page\n",
    "    \n",
    "    returns:\n",
    "        An integer with the amount of events\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html5lib')\n",
    "\n",
    "    tables = soup.select('table')\n",
    "\n",
    "    # The second table of depth three holds the amount of events\n",
    "    depth3 = [t for t in tables if len(t.find_parents('table')) == 3]\n",
    "    # The only bold element holds the amount of events\n",
    "    event_str = depth3[1].select('b')[0].text.splitlines()[0]\n",
    "\n",
    "    reg_exp = r'Showing .+ of (?P<events>\\d+)'\n",
    "    m = re.search(reg_exp, str(event_str))\n",
    "    no_events = int(m.group('events'))\n",
    "    \n",
    "    return no_events\n",
    "\n",
    "\n",
    "base_url = 'http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&periode=&Genre=Musik' \n",
    "no_events = scrape_no_events(base_url)\n",
    "print('Scraping {} events...'.format(no_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can scrape the events per page. Observe, that now, out `base_url` http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?Startnr={}&showmap=&Area=Kbh.%20og%20Frederiksberg&periode=&Genre=Musik& has a placeholder for the paginated results (`Startnr=`).\n",
    "\n",
    "Consequently, we scrape each page separately, see the function `scrape_events_per_page`. From examining the page's source code, we know that events are all given as table entries with a corresponding header. We iterate over each of the table cells and extract the strings for dates and prices if they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "    \n",
    "def scrape_events_per_page(url):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        A list of tuples of strings holding title, place, date, and price\n",
    "        for concerts in Copenhagen scraped from Kulturnaut.dk\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html5lib')\n",
    "    event_cells = soup.find_all('td', {'width': '100%', 'valign' : 'top'})\n",
    "    scraped_events_per_page = []\n",
    "    for event_cell in event_cells:\n",
    "        try:\n",
    "            title = event_cell.find('b').text\n",
    "            spans = event_cell.find_all('span')\n",
    "            place = spans[1].text\n",
    "            try:\n",
    "                date, price = spans[0].text.splitlines()\n",
    "            except ValueError as e:\n",
    "                date = spans[0].text.splitlines()[0]\n",
    "                price = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "        scraped_events_per_page.append((title, place, date, price))\n",
    "        \n",
    "    return scraped_events_per_page\n",
    "\n",
    "\n",
    "base_url = 'http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?Startnr={}&showmap=&Area=Kbh.%20og%20Frederiksberg&periode=&Genre=Musik&'\n",
    "\n",
    "scraped_events = []\n",
    "indexes = list(range(1, no_events, 20))\n",
    "indexes[0] = 0\n",
    "\n",
    "for idx in tqdm(indexes):\n",
    "    scrape_url = base_url.format(idx)\n",
    "    scraped_events += scrape_events_per_page(scrape_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we have so far?\n",
    "\n",
    "Now, you can see that we extracted a list of four element string tuples consisting of the title of the event, its location, a date and a time, and an entrance fee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_events[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Extract Dates and Prices from Strings.\n",
    "\n",
    "Remember, the raw data, which we extracted from the web pages is all of type `str`. To do statistics about possible correlation of start times and entry fees, we need to convert the corresponding tuple fields into datetimes and integers respectively.\n",
    "\n",
    "\n",
    "Since dates given on the web do not necessarily conform to standardized time formats, we can apply the `dateparser` (https://pypi.python.org/pypi/dateparser) module, which tries to parse arbitrary strings into datetimes.\n",
    "\n",
    "You can install the module via:\n",
    "\n",
    "```bash\n",
    "pip install dateparser\n",
    "```\n",
    "\n",
    "You can read more about the module and its capabilities https://dateparser.readthedocs.io/en/latest/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dateparser import parse\n",
    "\n",
    "\n",
    "def get_dates_and_prices(scraped_events):\n",
    "    \"\"\"\n",
    "    Cleanup the data. Get price as integer and date as date.\n",
    "    \n",
    "    returns:\n",
    "        A two-element tuple with a datetime representing the start \n",
    "        time of an event and an integer representing the price in Dkk.\n",
    "    \"\"\"\n",
    "\n",
    "    price_regexp = r\"(?P<price>\\d+)\"\n",
    "\n",
    "    data_points = []\n",
    "\n",
    "    for event_data in tqdm(scraped_events):\n",
    "        title_str, place_str, date_str, price_str = event_data\n",
    "        \n",
    "        if 'Free admission' in price_str:\n",
    "            price = 0\n",
    "        else:\n",
    "            m = re.search(price_regexp, price_str)\n",
    "            try:\n",
    "                price = int(m.group('price'))\n",
    "            except:\n",
    "                price = 0\n",
    "\n",
    "        date_str = date_str.strip().strip('.')\n",
    "        if '&' in date_str:\n",
    "            date_str = date_str.split('&')[0]\n",
    "        if '-' in date_str:\n",
    "            date_str = date_str.split('-')[0]\n",
    "        if '.' in date_str:\n",
    "            date_str = date_str.replace('.', ':')\n",
    "        \n",
    "        date = parse(date_str)\n",
    "        if date:\n",
    "            data_points.append((date, price))\n",
    "            \n",
    "    return data_points\n",
    "\n",
    "\n",
    "dates_and_prices = get_dates_and_prices(scraped_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_and_prices[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Times vs. Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dates, prices = zip(*dates_and_prices)\n",
    "ref_day = datetime.today()\n",
    "times = [datetime.combine(ref_day, t.time()) for t in dates]\n",
    "\n",
    "# plot\n",
    "plt.plot(times, prices, 'ro')\n",
    "# beautify the x-labels\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pearson Correlation Coefficient\n",
    "\n",
    "The Pearson correlation coefficient measures the linear relationship\n",
    "between two datasets. Like other correlation coefficients, this one varies between -1 and +1\n",
    "with 0 implying no correlation. Correlations of -1 or +1 imply an exact\n",
    "linear relationship. Positive correlations imply that as x increases, so\n",
    "does y. Negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "The p-value roughly indicates the probability of an uncorrelated system\n",
    "producing datasets that have a Pearson correlation at least as extreme\n",
    "as the one computed from these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "x, y = zip(*dates_and_prices)\n",
    "x = matplotlib.dates.date2num(x)\n",
    "\n",
    "pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, since our Pearson Correlation Coefficient and the corresponding p-value are quite close to zero, there is likely no correlation between the start time of a concert and the price you have to pay for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Images from a Page\n",
    "\n",
    "In the following just another small example on how to use Beautiful Soup. Here, we extract all links to images, which are in `img` tags on a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "\n",
    "def collect_img_links(url):\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html5lib')\n",
    "    \n",
    "    return [img.get('src') for img in soup.select('img') \n",
    "            if img.get('src').startswith('http')]\n",
    "\n",
    "\n",
    "def download_imgs(links, out_folder=\"./test/\"):\n",
    "    for l in links:\n",
    "        # You know how to do this!\n",
    "        pass \n",
    "        \n",
    "        \n",
    "collect_img_links('https://www.google.dk/search?site=&tbm=isch&source=hp&biw=1163&bih=812&q=minions&oq=minions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a Simple Web Crawler\n",
    "\n",
    "As last example, we will write a simple web crawler. More precisely, a program that extracts recursively all links from web pages. The result of running the web crawler is a dictionary, were the key-value pairs correspond to outgoiung links from a web page with the URL, which is stored in the key.\n",
    "\n",
    "\n",
    "In case a page returns a status code, which is not `200` we just disregard this page. See https://en.wikipedia.org/wiki/List_of_HTTP_status_codes for more detailes on the various HTTP status codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bs4\n",
    "import pprint\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "seen_that = set([])\n",
    "skipped_that = set([])\n",
    "\n",
    "\n",
    "def scrape_links_of_page(url):\n",
    "    \"\"\"\n",
    "    url:\n",
    "        The URL as string of the page under consideration.\n",
    "    returns:\n",
    "        A list of tuples of strings holding title, place, date, and price\n",
    "        for concerts in Copenhagen scraped from Kulturnaut.dk\n",
    "    \"\"\"\n",
    "\n",
    "    scraped_links = set([])\n",
    "    if not url in seen_that:\n",
    "        seen_that.update([url])\n",
    "        try:\n",
    "            h = requests.head(url, verify=False)\n",
    "            content_type = h.headers.get('content-type')\n",
    "            if content_type and content_type.startswith('text/html'):\n",
    "                print('Scraping ', url)\n",
    "                \n",
    "                r = requests.get(url, verify=False)\n",
    "                if r.status_code == 200:\n",
    "                    soup = bs4.BeautifulSoup(r.text)\n",
    "\n",
    "                    for link in soup.select('a'):\n",
    "                        possible_link = link.get('href')\n",
    "                        if possible_link and isinstance(possible_link, str) and (\n",
    "                            possible_link.startswith('http')):\n",
    "                            scraped_links.update([possible_link])\n",
    "                else:\n",
    "                    print('Skipped {} ... got {}'.format(r.url, r.status_code))\n",
    "                    skipped_that.update([r.url])\n",
    "            else:\n",
    "                print('Skipped {} ... since content-type {}'.format(url, content_type))\n",
    "                skipped_that.update([url])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            skipped_that.update([url])\n",
    "    return {url: list(scraped_links)}\n",
    "\n",
    "\n",
    "def scrape_links_of_pages(urls):\n",
    "    out_links = {}\n",
    "    for url in tqdm(urls):\n",
    "    # for url in urls:\n",
    "        link_edges = scrape_links_of_page(url)\n",
    "        out_links.update(link_edges)\n",
    "        \n",
    "    return out_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(from_url, for_depth, all_links={}):\n",
    "    # This is what the exercise below asks you to implement!\n",
    "    pass\n",
    "\n",
    "\n",
    "start_url = 'https://www.version2.dk/artikel/google-deepmind-vi-oeger-sikkerheden-mod-misbrug-sundhedsdata-1074452'\n",
    "\n",
    "link_dict = scrape_links(from_url=start_url, for_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(a_dict):\n",
    "    with open('./internet.py', 'w') as out_file:\n",
    "        out_file.write('LINKS_DICT =' + pprint.pformat(a_dict))\n",
    "\n",
    "\n",
    "def flatten(lst):\n",
    "    import itertools\n",
    "    return list(itertools.chain.from_iterable(lst))\n",
    "\n",
    "\n",
    "def scrape_links(from_url, for_depth, all_links={}):\n",
    "\n",
    "    if for_depth >= 1:\n",
    "        print('Scraping level {}'.format(for_depth))\n",
    "        \n",
    "        if isinstance(from_url, list):\n",
    "            if all_links:\n",
    "                already_seen = list(all_links.keys())\n",
    "                still_to_scrape = [l for l in from_url if not l in already_seen]\n",
    "            else:\n",
    "                still_to_scrape = from_url\n",
    "\n",
    "        elif isinstance(from_url, str):\n",
    "            if all_links:\n",
    "                already_seen = list(all_links.keys())\n",
    "                still_to_scrape = [l for l in [from_url] if not l in all_links.keys()]\n",
    "            else:\n",
    "                still_to_scrape = [from_url]\n",
    "\n",
    "        edges_dict = scrape_links_of_pages(still_to_scrape)\n",
    "        \n",
    "        for_depth -= 1\n",
    "        \n",
    "        all_links.update(edges_dict)\n",
    "\n",
    "        values = flatten(edges_dict.values())\n",
    "        #dump(all_links)\n",
    "        scrape_links(values, for_depth, all_links=all_links)\n",
    "    else:\n",
    "        print('Done')\n",
    "        \n",
    "    return all_links\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = 'https://www.version2.dk/artikel/google-deepmind-vi-oeger-sikkerheden-mod-misbrug-sundhedsdata-1074452'\n",
    "\n",
    "link_dict = scrape_links(from_url=start_url, for_depth=2)\n",
    "\n",
    "print('{} pages link to {} other pages'.format(len(list(link_dict.keys())), \n",
    "                                               len(flatten(link_dict.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "39069 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = 'https://github.com/HelgeCPH/get_things_done_with_python/blob/master/notebooks/02-Getting%20Started.ipynb'\n",
    "link_dict = scrape_links(from_url=start_url, for_depth=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://github-enterprise.s3.amazonaws.com/kvm/releases/github-enterprise-2.9.0.qcow2'\n",
    "url = ' https://github.com/sschuberth'\n",
    "\n",
    "h = requests.head(url, verify=False)\n",
    "content_type = h.headers.get('content-type')\n",
    "print(content_type)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web crawler that you wrote above is perhaps not the most performant. If you are interested in more web scraping and application of crawlers have a look at the `scrapy` module (https://scrapy.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading files Memory-friendly with Generator Functions\n",
    "\n",
    "\n",
    "## Reading Memory Unfriendly:\n",
    "\n",
    "Reading all the data into RAM and process it afterwards...\n",
    "\n",
    "```python\n",
    "def read_mem_unfriendly(path_to_file):\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "i = 0\n",
    "for line in read_mem_unfriendly('my_file.txt'):\n",
    "    # Do something with line\n",
    "    i += 1\n",
    "```\n",
    "\n",
    "\n",
    "## Reading Memory Friendly\n",
    "\n",
    "The following implementation uses a generator, consequently the program keeps only one line at the time in memory. See https://stackoverflow.com/a/1756156 for more details on generators.\n",
    "\n",
    "```python\n",
    "def read_mem_friendly(path_to_file):\n",
    "    \n",
    "    with open(path_to_file) as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "\n",
    "i = 0\n",
    "for line in read_mem_friendly('my_file.txt'):\n",
    "    # Do something with line\n",
    "    i += 1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
