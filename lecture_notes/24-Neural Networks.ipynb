{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watson Analytics\n",
    "\n",
    "Watson Analytics is a Cloud Service offering AI capabilities via REST APIs. https://www.ibm.com/analytics/watson-analytics/us-en/index.html\n",
    "\n",
    "\n",
    "https://github.com/watson-developer-cloud/python-sdk\n",
    "\n",
    "```bash\n",
    "pip install watson-developer-cloud\n",
    "```\n",
    "The API documentation is here:\n",
    "https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/curl.html?curl\n",
    "\n",
    "\n",
    "## Getting the Service Credentials\n",
    "\n",
    "Service credentials are required to access the APIs.\n",
    "\n",
    "To run locally or outside of Bluemix you need the `username` and `password` credentials for each service. (Service credentials are different from your Bluemix account email and password.)\n",
    "\n",
    "To create an instance of the service:\n",
    "\n",
    "  * Log in to [Bluemix](https://console.ng.bluemix.net).\n",
    "  * Create an instance of the service:\n",
    "    1. In the Bluemix Catalog, select the Watson service you want to use. For our example, select under *Watson* the *Visual Recognition* service.\n",
    "    2. Type a unique name for the service instance in the Service name field. For example, type my-service-name. Leave the default values for the other options.\n",
    "    3. Click Create.\n",
    "    \n",
    "    \n",
    "To get your service credentials:\n",
    "\n",
    "  * Copy your credentials from the *Service Details* page. To find the the *Service Details* page for an existing service, navigate to your Bluemix dashboard and click the service name.\n",
    "  * On the *Service Details* page, click *Service Credentials*, and then *View Credentials*.\n",
    "Copy username and password.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"IBM_WATSON_API_KEY='YOUR_API_KEY'\" >> ./api_keys.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import api_keys\n",
    "\n",
    "\n",
    "API_KEY = {\n",
    "  \"url\": \"https://gateway-a.watsonplatform.net/visual-recognition/api\",\n",
    "  \"note\": \"It may take up to 5 minutes for this key to become active\",\n",
    "  \"api_key\": api_keys.IBM_WATSON_API_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Network with Pre-trained Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See example:\n",
    "# https://github.com/watson-developer-cloud/python-sdk/blob/master/examples/visual_recognition_v3.py\n",
    "import json\n",
    "from watson_developer_cloud import VisualRecognitionV3\n",
    "\n",
    "\n",
    "test_url = 'https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg'\n",
    "\n",
    "visual_recognition = VisualRecognitionV3('2016-05-20', api_key=API_KEY['api_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_recognition.classify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the Watson's Visual Recognition Classifier says to IBM's CEO Ginni Rometty.\n",
    "![](https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"womans portrait photo\",\n",
      "              \"score\": 0.599,\n",
      "              \"type_hierarchy\": \"/person/female/woman/womans portrait photo\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"woman\",\n",
      "              \"score\": 0.601\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"female\",\n",
      "              \"score\": 0.602\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.725\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"adult person\",\n",
      "              \"score\": 0.559,\n",
      "              \"type_hierarchy\": \"/person/adult person\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"people\",\n",
      "              \"score\": 0.5,\n",
      "              \"type_hierarchy\": \"/person/people\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"light brown color\",\n",
      "              \"score\": 0.718\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"beige color\",\n",
      "              \"score\": 0.533\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\",\n",
      "      \"resolved_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.classify(url=test_url), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_recognition.detect_faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 48,\n",
      "            \"max\": 51,\n",
      "            \"score\": 0.76694834\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 245,\n",
      "            \"width\": 237,\n",
      "            \"left\": 286,\n",
      "            \"top\": 177\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99999833\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\",\n",
      "      \"resolved_url\": \"https://www.ibm.com/ibm/ginni/images/ginni_bio_780x981_v4_03162016.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.detect_faces(parameters=json.dumps({'url': test_url})), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classifiers\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.list_classifiers(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read more about what is possible via the Python API, see https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/?python.\n",
    "\n",
    "In essence, the Python API is just wrapping HTTP REST API calls with the `requests` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to classify a local image?\n",
    "\n",
    "There are many labeled datasets used in image recognition research. One of them is the Caltech 101 dataset, see http://www.vision.caltech.edu/Image_Datasets/Caltech101/. The actual dataset is here: http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\n",
    "\n",
    "\n",
    "After downloading and and uncompressing the dataset, we can send the image of a butterfly to Watson.\n",
    "![](images/image_0027.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"monarch butterfly\",\n",
      "              \"score\": 0.87,\n",
      "              \"type_hierarchy\": \"/animal/invertebrate/insect/butterfly/monarch butterfly\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"butterfly\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"insect\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"invertebrate\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"animal\",\n",
      "              \"score\": 0.977\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"viceroy butterfly\",\n",
      "              \"score\": 0.782,\n",
      "              \"type_hierarchy\": \"/animal/invertebrate/insect/butterfly/viceroy butterfly\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"reddish orange color\",\n",
      "              \"score\": 0.889\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"Indian red color\",\n",
      "              \"score\": 0.78\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"image\": \"./images/image_0027.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('./images/image_0027.jpg', 'rb') as image_file:\n",
    "    results = visual_recognition.classify(\n",
    "            images_file=image_file,\n",
    "            threshold='0.1')\n",
    "    print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "![](http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.825\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"people\",\n",
      "              \"score\": 0.5,\n",
      "              \"type_hierarchy\": \"/person/people\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"Indian red color\",\n",
      "              \"score\": 0.97\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg\",\n",
      "      \"resolved_url\": \"http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mister_t_url = 'http://www.xpautographs.com/11337-7718-thickbox/mister-t-autograph.jpg'\n",
    "print(json.dumps(visual_recognition.classify(url=mister_t_url), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.cphbusiness.dk/media/75910/lam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"walrus mustache\",\n",
      "              \"score\": 0.559,\n",
      "              \"type_hierarchy\": \"/person/walrus mustache\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.811\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"beard\",\n",
      "              \"score\": 0.538,\n",
      "              \"type_hierarchy\": \"/person/beard\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"mustachio\",\n",
      "              \"score\": 0.511,\n",
      "              \"type_hierarchy\": \"/person/mustachio\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person portrait photo\",\n",
      "              \"score\": 0.51,\n",
      "              \"type_hierarchy\": \"/person/person/person portrait photo\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"taxonomist\",\n",
      "              \"score\": 0.504,\n",
      "              \"type_hierarchy\": \"/person/taxonomist\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"adult person\",\n",
      "              \"score\": 0.5,\n",
      "              \"type_hierarchy\": \"/person/adult person\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"reddish brown color\",\n",
      "              \"score\": 0.742\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.cphbusiness.dk/media/75910/lam.png\",\n",
      "      \"resolved_url\": \"https://www.cphbusiness.dk/media/75910/lam.png\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "lars_url = 'https://www.cphbusiness.dk/media/75910/lam.png'\n",
    "print(json.dumps(visual_recognition.classify(url=lars_url), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.cphbusiness.dk/media/74691/ltje.jpg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 25,\n",
      "            \"max\": 28,\n",
      "            \"score\": 0.79470074\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 949,\n",
      "            \"width\": 832,\n",
      "            \"left\": 333,\n",
      "            \"top\": 514\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.999997\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.cphbusiness.dk/media/74691/ltje.jpg\",\n",
      "      \"resolved_url\": \"https://www.cphbusiness.dk/media/74691/ltje.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "line_url = 'https://www.cphbusiness.dk/media/74691/ltje.jpg'\n",
    "print(json.dumps(visual_recognition.detect_faces(parameters=json.dumps({'url': line_url})), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Many Faces in an Image\n",
    "\n",
    "![](http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"name\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"wraparound\",\n",
      "              \"score\": 0.548,\n",
      "              \"type_hierarchy\": \"/garment/wraparound\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"garment\",\n",
      "              \"score\": 0.548\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"family\",\n",
      "              \"score\": 0.525,\n",
      "              \"type_hierarchy\": \"/person/family\"\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"score\": 0.526\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"people\",\n",
      "              \"score\": 0.563\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"group of people\",\n",
      "              \"score\": 0.5\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"alizarine red color\",\n",
      "              \"score\": 0.846\n",
      "            },\n",
      "            {\n",
      "              \"class\": \"ivory color\",\n",
      "              \"score\": 0.724\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\",\n",
      "      \"resolved_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1,\n",
      "  \"custom_classes\": 0\n",
      "}\n",
      "{\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 32,\n",
      "            \"max\": 35,\n",
      "            \"score\": 0.8495456\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 49,\n",
      "            \"width\": 45,\n",
      "            \"left\": 173,\n",
      "            \"top\": 91\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.9584368\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 51,\n",
      "            \"max\": 55,\n",
      "            \"score\": 0.6513539\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 52,\n",
      "            \"width\": 46,\n",
      "            \"left\": 239,\n",
      "            \"top\": 68\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.99932075\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 23,\n",
      "            \"max\": 26,\n",
      "            \"score\": 0.7988043\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 45,\n",
      "            \"width\": 36,\n",
      "            \"left\": 152,\n",
      "            \"top\": 50\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.9971029\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 22,\n",
      "            \"max\": 26,\n",
      "            \"score\": 0.73620075\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 42,\n",
      "            \"width\": 40,\n",
      "            \"left\": 31,\n",
      "            \"top\": 94\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99993646\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 19,\n",
      "            \"max\": 22,\n",
      "            \"score\": 0.99972486\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 55,\n",
      "            \"width\": 41,\n",
      "            \"left\": 190,\n",
      "            \"top\": 136\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.999127\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 20,\n",
      "            \"max\": 24,\n",
      "            \"score\": 0.6713774\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 46,\n",
      "            \"width\": 38,\n",
      "            \"left\": 52,\n",
      "            \"top\": 144\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.9985127\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 23,\n",
      "            \"max\": 26,\n",
      "            \"score\": 0.84344494\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 49,\n",
      "            \"width\": 48,\n",
      "            \"left\": 228,\n",
      "            \"top\": 145\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99955493\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 28,\n",
      "            \"max\": 33,\n",
      "            \"score\": 0.51941246\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 48,\n",
      "            \"width\": 33,\n",
      "            \"left\": 109,\n",
      "            \"top\": 134\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.99695385\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"age\": {\n",
      "            \"min\": 25,\n",
      "            \"max\": 28,\n",
      "            \"score\": 0.9299795\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"height\": 46,\n",
      "            \"width\": 44,\n",
      "            \"left\": 45,\n",
      "            \"top\": 47\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.99030745\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\",\n",
      "      \"resolved_url\": \"http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg\"\n",
      "    }\n",
      "  ],\n",
      "  \"images_processed\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "group_url = 'http://www.albanyjobfair.com/wp-content/uploads/2014/01/BIz-people-2-300x276.jpg'\n",
    "\n",
    "print(json.dumps(visual_recognition.classify(url=group_url), indent=2))\n",
    "print(json.dumps(visual_recognition.detect_faces(parameters=json.dumps({'url': group_url})), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training your own Classifier\n",
    "\n",
    "This is your task now. Take some images from different categories from the Caltech 101 dataset and train the neural network with them. For example, as in the following:\n",
    "\n",
    "```python\n",
    "with open('/path/to/butterflies.zip', 'rb') as butterflies, \\\n",
    "     open('/path/to/airplanes.zip'), 'rb') as airplanes:\n",
    "    print(json.dumps(visual_recognition.create_classifier('ButterfliesvsPlanes', \n",
    "                                                          butterflies_positive_examples=butterflies, \n",
    "                                                          negative_examples=airplanes), \n",
    "                     indent=2))\n",
    "```\n",
    "\n",
    "When you created some classifiers, you can list them as in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classifiers\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.list_classifiers(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does this work? Introduction to Neural Networks.\n",
    "\n",
    "\n",
    "A next step from a single perceptron -as seen in the last lecture- to an image classifier as above is a Multi-layer Perceptron.\n",
    "\n",
    "![](http://www.saedsayad.com/images/Perceptron_bkp_1.png)\n",
    "\n",
    "\n",
    "\n",
    "The code in the following is adapted from Chapter 18 \"Neural Networks\" in the Data Science from Scratch book. The code can be found at: https://github.com/joelgrus/data-science-from-scratch/blob/master/code-python3/neural_networks.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/20000 [00:00<00:40, 491.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:42<00:00, 470.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "random.seed(0)   # to get repeatable results\n",
    "input_size = 25  # each input is a vector of length 25\n",
    "\n",
    "num_hidden = 5   # we'll have 5 neurons in the hidden layer\n",
    "output_size = 10 # we need 10 outputs for each input\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for _ in range(input_size + 1)]\n",
    "                 for _ in range(num_hidden)]\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for _ in range(num_hidden + 1)]\n",
    "                 for _ in range(output_size)]\n",
    "\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]\n",
    "\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "# 10,000 iterations seems enough to converge\n",
    "for _ in tqdm(range(20000)):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Trainings Dataset\n",
    "\n",
    "We want to create a Multi-layer Perceptron, which can classifiy -or recognize- the digits from zero to nine for us. In `raw_digits` we create digits consisting out of 5x5 binary pixels. Consequently, each input in our trainings dataset is a binary vector of length 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_digits = [\n",
    "        \"\"\"11111\n",
    "           1...1\n",
    "           1...1\n",
    "           1...1\n",
    "           11111\"\"\",\n",
    "        \"\"\"..1..\n",
    "           ..1..\n",
    "           ..1..\n",
    "           ..1..\n",
    "           ..1..\"\"\",\n",
    "        \"\"\"11111\n",
    "           ....1\n",
    "           11111\n",
    "           1....\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           ....1\n",
    "           11111\n",
    "           ....1\n",
    "           11111\"\"\",\n",
    "        \"\"\"1...1\n",
    "           1...1\n",
    "           11111\n",
    "           ....1\n",
    "           ....1\"\"\",\n",
    "        \"\"\"11111\n",
    "           1....\n",
    "           11111\n",
    "           ....1\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           1....\n",
    "           11111\n",
    "           1...1\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           ....1\n",
    "           ....1\n",
    "           ....1\n",
    "           ....1\"\"\",\n",
    "        \"\"\"11111\n",
    "           1...1\n",
    "           11111\n",
    "           1...1\n",
    "           11111\"\"\",\n",
    "        \"\"\"11111\n",
    "           1...1\n",
    "           11111\n",
    "           ....1\n",
    "           11111\"\"\"]\n",
    "\n",
    "\n",
    "def make_digit(raw_digit):\n",
    "    return [1 if c == '1' else 0\n",
    "            for row in raw_digit.split(\"\\n\")\n",
    "            for c in row.strip()]\n",
    "\n",
    "\n",
    "inputs = [make_digit(raw_digit) for raw_digit in raw_digits]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = [[1 if i == j else 0 for i in range(10)] for j in range(10)]\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convience and reusabilty, we save the vectors containing the raw digits to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1]\n",
      " [1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1]\n",
      " [1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "print(np.array(inputs, dtype=np.int8))\n",
    "#np.savetxt?\n",
    "np.savetxt('./simple_digit_trainingset.csv', np.array(inputs, dtype=np.int8), delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,1,1,1,1,0,0,0,1,1,0,0,0,1,1,0,0,0,1,1,1,1,1,1\n",
      "0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0\n",
      "1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1\n",
      "1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1\n",
      "1,0,0,0,1,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,1\n",
      "1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1\n",
      "1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1\n",
      "1,1,1,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1\n",
      "1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1\n",
      "1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat simple_digit_trainingset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create to helper functions, one for reading our trainings dataset from a file and a second one, which will plot it for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f4b4748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import csv\n",
    "import webget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "filename = './simple_digit_trainingset.csv'\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            label = reader.line_num - 1\n",
    "            image = np.array(row[:], dtype=np.int8)\n",
    "            data.append((label, image))\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_plot(data):\n",
    "    count = 0\n",
    "    f = plt.figure(figsize=(10, 5))\n",
    "    for idx, row in enumerate(data):\n",
    "        imarray = row[1].reshape((5, 5))\n",
    "        plt.subplot(2, 5, idx + 1)\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        count += 1\n",
    "        plt.title('Label = {}'.format(row[0]))\n",
    "        plt.imshow(imarray, cmap='Greys', interpolation='None')\n",
    "    return plt\n",
    "\n",
    "\n",
    "trainings_set = read_data(filename)\n",
    "plt.show(generate_plot(trainings_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network (represented as a list of lists of lists of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "\n",
    "        input_with_bias = input_vector + [1]             # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias) # compute the output\n",
    "                  for neuron in layer]                   # for this layer\n",
    "        outputs.append(output)                           # and remember it\n",
    "\n",
    "        # the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def predict(in_put):\n",
    "    return feed_forward(network, in_put)[-1]\n",
    "\n",
    "\n",
    "def backpropagate(network, input_vector, target):\n",
    "\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target[i])\n",
    "                     for i, output in enumerate(outputs)]\n",
    "\n",
    "    # adjust weights for output layer (network[-1])\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            # print(i,j)\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "    #print('----')\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                      dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # adjust weights for hidden layer (network[0])\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, in_put in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * in_put"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Test-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0], [0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ed9bd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_testset(data):\n",
    "    count = 0\n",
    "    f = plt.figure(figsize=(10, 5))\n",
    "    data = np.array(data)\n",
    "    for idx, row in enumerate(data):\n",
    "        imarray = row.reshape((5, 5))\n",
    "        plt.subplot(2, len(data), idx + 1)\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        count += 1\n",
    "        plt.imshow(imarray, cmap='Greys', interpolation='None')\n",
    "    return plt\n",
    "\n",
    "\n",
    "test_set = [[0,1,1,1,0,\n",
    "             0,0,0,1,1,\n",
    "             0,0,1,1,0,\n",
    "             0,0,0,1,1,\n",
    "             0,1,1,1,0],\n",
    "            [0,1,1,1,0,\n",
    "             1,0,0,1,1,\n",
    "             0,1,1,1,0,\n",
    "             1,0,0,1,1,\n",
    "             0,1,1,1,0],\n",
    "            [0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0],\n",
    "            [0,1,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0,\n",
    "             0,0,1,0,0]]\n",
    "print(test_set)\n",
    "plt.show(plot_testset(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [0.   0.   0.   0.98 0.   0.   0.   0.01 0.   0.04]\n",
      "9 [0.   0.   0.   0.   0.   0.6  0.   0.   0.93 1.  ]\n",
      "1 [0.   0.98 0.02 0.01 0.01 0.   0.   0.   0.   0.  ]\n",
      "3 [0.   0.19 0.   0.76 0.   0.   0.   0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "for test_data in test_set:\n",
    "    result = predict(test_data)\n",
    "    result = np.array(result)\n",
    "    print(np.argmax(result), np.array_str(result, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 [0.   0.   0.   0.93 0.   0.   0.   0.01 0.   0.1 ]\n",
    "9 [0.   0.   0.   0.   0.   0.54 0.   0.   0.91 1.  ]\n",
    "1 [0.   0.96 0.03 0.02 0.   0.   0.   0.   0.   0.  ]\n",
    "3 [0.   0.22 0.   0.73 0.   0.   0.   0.   0.   0.  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.97 0.   0.   0.   0.   0.   0.   0.02 0.02 0.  ]\n",
      "1 [0.   0.98 0.02 0.01 0.01 0.   0.   0.   0.   0.  ]\n",
      "2 [0.   0.02 0.97 0.   0.   0.02 0.   0.   0.   0.  ]\n",
      "3 [0.   0.02 0.   0.98 0.   0.   0.   0.01 0.   0.02]\n",
      "4 [0.   0.01 0.01 0.   0.99 0.   0.   0.   0.   0.  ]\n",
      "5 [0.   0.   0.02 0.   0.   0.97 0.01 0.   0.01 0.01]\n",
      "6 [0.   0.   0.01 0.   0.01 0.01 0.99 0.   0.01 0.  ]\n",
      "7 [0.02 0.   0.   0.01 0.   0.   0.   0.98 0.   0.  ]\n",
      "8 [0.02 0.   0.   0.   0.   0.01 0.   0.   0.97 0.02]\n",
      "9 [0.   0.   0.   0.01 0.   0.01 0.   0.   0.02 0.97]\n"
     ]
    }
   ],
   "source": [
    "for test_data in inputs:\n",
    "    result = predict(test_data)\n",
    "    result = np.array(result)\n",
    "    print(np.argmax(result), np.array_str(result, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Done Properly...\n",
    "\n",
    "And we still have a bit to go from classifying digits with a Multi-layer Perceptron to a Convolutional Neural Network (CNN), which is the technique that IBM applies in Watson for visual recognition. A modern framework for implementing various types of neural networks is Google's Tensorflow.\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "You can get more information about it here:\n",
    "\n",
    "  * https://www.youtube.com/watch?v=qyvlt7kiQoI\n",
    "  * https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0\n",
    "  * https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py\n",
    "  * https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise!!!\n",
    "\n",
    "Your task is to extend the above example to work with the 'classical' MNIST dataset, which contains many thousands of handwritten digits. Your task is to watch the video https://www.youtube.com/watch?v=qyvlt7kiQoI on YouTube, which gives an introduction to Google's Tensorflow -a Python framework helping to build neural networks- and you follow the tutorial on https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist.\n",
    "\n",
    "You have to reproduce their solution and the results at least until step 6 (including step 6) and futher if you feel like it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Possible Projects:\n",
    "\n",
    "  * Implementation of a Salient Region Detector: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.228.5552&rep=rep1&type=pdf\n",
    "  * Audio Fingerprinting: http://willdrevo.com/fingerprinting-and-audio-recognition-with-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
